{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategies in Federated Learning\n",
    "\n",
    "Welcome to the next part of the federated learning tutorial. In previous parts of this tutorial, we introduced federated learning with PyTorch and Flower ([part 1](https://flower.dev/docs/tutorial/Flower-1-Intro-to-FL-PyTorch.html)).\n",
    "\n",
    "In this notebook, we'll begin to customize the federated learning system we built in the introductory notebook (again, using [Flower](https://flower.dev/) and [PyTorch](https://pytorch.org/)).\n",
    "\n",
    "> Join the Flower community on Slack to connect, ask questions, and get help: [Join Slack](https://flower.dev/join-slack) ðŸŒ» We'd love to hear from you in the `#introductions` channel! If anything is unclear, head over to the `#questions` channel.\n",
    "\n",
    "Let's move beyond FedAvg with Flower Strategies!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "Before we begin with the actual code, let's make sure that we have everything we need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing dependencies\n",
    "\n",
    "First, we install the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q flwr[simulation] torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all dependencies installed, we can import everything we need for this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu using PyTorch 1.13.1+cu117 and Flower 1.3.0\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "import flwr as fl\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")  # Try \"cuda\" to train on GPU\n",
    "print(\n",
    "    f\"Training on {DEVICE} using PyTorch {torch.__version__} and Flower {fl.__version__}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to switch to a runtime that has GPU acceleration enabled (on Google Colab: `Runtime > Change runtime type > Hardware acclerator: GPU > Save`). Note, however, that Google Colab is not always able to offer GPU acceleration. If you see an error related to GPU availability in one of the following sections, consider switching back to CPU-based execution by setting `DEVICE = torch.device(\"cpu\")`. If the runtime has GPU acceleration enabled, you should see the output `Training on cuda`, otherwise it'll say `Training on cpu`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading\n",
    "\n",
    "Let's now load the CIFAR-10 training and test set, partition them into ten smaller datasets (each split into training and validation set), and wrap everything in their own `DataLoader`. We introduce a new parameter `num_clients` which allows us to call `load_datasets` with different numbers of clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "NUM_CLIENTS = 10\n",
    "\n",
    "\n",
    "def load_datasets(num_clients: int):\n",
    "    # Download and transform CIFAR-10 (train and test)\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "    )\n",
    "    trainset = CIFAR10(\"./dataset\", train=True, download=True, transform=transform)\n",
    "    testset = CIFAR10(\"./dataset\", train=False, download=True, transform=transform)\n",
    "\n",
    "    # Split training set into `num_clients` partitions to simulate different local datasets\n",
    "    partition_size = len(trainset) // num_clients\n",
    "    lengths = [partition_size] * num_clients\n",
    "    datasets = random_split(trainset, lengths, torch.Generator().manual_seed(42))\n",
    "\n",
    "    # Split each partition into train/val and create DataLoader\n",
    "    trainloaders = []\n",
    "    valloaders = []\n",
    "    for ds in datasets:\n",
    "        len_val = len(ds) // 10  # 10 % validation set\n",
    "        len_train = len(ds) - len_val\n",
    "        lengths = [len_train, len_val]\n",
    "        ds_train, ds_val = random_split(ds, lengths, torch.Generator().manual_seed(42))\n",
    "        trainloaders.append(DataLoader(ds_train, batch_size=32, shuffle=True))\n",
    "        valloaders.append(DataLoader(ds_val, batch_size=32))\n",
    "    testloader = DataLoader(testset, batch_size=32)\n",
    "    return trainloaders, valloaders, testloader\n",
    "\n",
    "\n",
    "trainloaders, valloaders, testloader = load_datasets(NUM_CLIENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training/evaluation\n",
    "\n",
    "Let's continue with the usual model definition (including `set_parameters` and `get_parameters`), training and test functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def get_parameters(net) -> List[np.ndarray]:\n",
    "    return [val.cpu().numpy() for _, val in net.state_dict().items()]\n",
    "\n",
    "\n",
    "def set_parameters(net, parameters: List[np.ndarray]):\n",
    "    params_dict = zip(net.state_dict().keys(), parameters)\n",
    "    state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "    net.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "\n",
    "def train(net, trainloader, epochs: int):\n",
    "    \"\"\"Train the network on the training set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters())\n",
    "    net.train()\n",
    "    for epoch in range(epochs):\n",
    "        correct, total, epoch_loss = 0, 0, 0.0\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(images)\n",
    "            loss = criterion(net(images), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Metrics\n",
    "            epoch_loss += loss\n",
    "            total += labels.size(0)\n",
    "            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
    "        epoch_loss /= len(trainloader.dataset)\n",
    "        epoch_acc = correct / total\n",
    "        print(f\"Epoch {epoch+1}: train loss {epoch_loss}, accuracy {epoch_acc}\")\n",
    "\n",
    "\n",
    "def test(net, testloader):\n",
    "    \"\"\"Evaluate the network on the entire test set.\"\"\"\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    correct, total, loss = 0, 0, 0.0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = net(images)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    loss /= len(testloader.dataset)\n",
    "    accuracy = correct / total\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flower client\n",
    "\n",
    "To implement the Flower client, we (again) create a subclass of `flwr.client.NumPyClient` and implement the three methods `get_parameters`, `fit`, and `evaluate`. Here, we also pass the `cid` to the client and use it log additional details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowerClient(fl.client.NumPyClient):\n",
    "    def __init__(self, cid, net, trainloader, valloader):\n",
    "        self.cid = cid\n",
    "        self.net = net\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        print(f\"[Client {self.cid}] get_parameters\")\n",
    "        return get_parameters(self.net)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        print(f\"[Client {self.cid}] fit, config: {config}\")\n",
    "        set_parameters(self.net, parameters)\n",
    "        train(self.net, self.trainloader, epochs=1)\n",
    "        return get_parameters(self.net), len(self.trainloader), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        print(f\"[Client {self.cid}] evaluate, config: {config}\")\n",
    "        set_parameters(self.net, parameters)\n",
    "        loss, accuracy = test(self.net, self.valloader)\n",
    "        return float(loss), len(self.valloader), {\"accuracy\": float(accuracy)}\n",
    "\n",
    "\n",
    "def client_fn(cid) -> FlowerClient:\n",
    "    net = Net().to(DEVICE)\n",
    "    trainloader = trainloaders[int(cid)]\n",
    "    valloader = valloaders[int(cid)]\n",
    "    return FlowerClient(cid, net, trainloader, valloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy customization\n",
    "\n",
    "So far, everything should look familiar if you've worked through the introductory notebook. With that, we're ready to introduce a number of new features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Server-side parameter **initialization**\n",
    "\n",
    "Flower, by default, initializes the global model by asking one random client for the initial parameters. In many cases, we want more control over parameter initialization though. Flower therefore allows you to directly pass the initial parameters to the Strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-02-27 15:47:42,851 | app.py:145 | Starting Flower simulation, config: ServerConfig(num_rounds=3, round_timeout=None)\n",
      "2023-02-27 15:47:58,512\tINFO worker.py:1529 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "E0227 15:48:09.053167700   14900 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {\"created\":\"@1677509289.052203500\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":202,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "INFO flwr 2023-02-27 15:48:09,445 | app.py:179 | Flower VCE: Ray initialized with resources: {'node:10.246.68.42': 1.0, 'memory': 734097408.0, 'object_store_memory': 367048704.0, 'CPU': 8.0}\n",
      "INFO flwr 2023-02-27 15:48:09,456 | server.py:86 | Initializing global parameters\n",
      "INFO flwr 2023-02-27 15:48:09,497 | server.py:266 | Using initial parameters provided by strategy\n",
      "INFO flwr 2023-02-27 15:48:09,537 | server.py:88 | Evaluating initial parameters\n",
      "INFO flwr 2023-02-27 15:48:09,555 | server.py:101 | FL starting\n",
      "DEBUG flwr 2023-02-27 15:48:09,573 | server.py:215 | fit_round 1: strategy sampled 3 clients (out of 10)\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m E0227 15:48:24.869935000   15770 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {\"created\":\"@1677509304.869896500\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":202,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m E0227 15:48:24.899927200   15760 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {\"created\":\"@1677509304.899875100\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":202,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m E0227 15:48:24.989847500   15771 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {\"created\":\"@1677509304.989816200\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":202,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-27 15:48:25,246 E 15232 15232] (raylet) local_object_manager.cc:360: Failed to send object spilling request: GrpcUnavailable: RPC Error message: Socket closed; RPC Error details: \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m E0227 15:48:29.358433700   15873 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {\"created\":\"@1677509309.358363500\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":202,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m E0227 15:48:32.266506500   15901 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {\"created\":\"@1677509312.266413300\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":202,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m E0227 15:48:32.628602700   15939 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {\"created\":\"@1677509312.628567800\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":202,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=15343)\u001b[0m [Client 8] fit, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m E0227 15:48:34.826798100   15990 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {\"created\":\"@1677509314.826763900\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":202,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=15343)\u001b[0m Epoch 1: train loss 0.06502389162778854, accuracy 0.22955555555555557\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=15343)\u001b[0m [Client 6] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=15345)\u001b[0m [Client 0] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=15345)\u001b[0m Epoch 1: train loss 0.0649348720908165, accuracy 0.2371111111111111\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=15345)\u001b[0m [Client 6] fit, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-02-27 15:48:54,495 | server.py:229 | fit_round 1 received 3 results and 0 failures\n",
      "WARNING flwr 2023-02-27 15:48:54,516 | fedavg.py:242 | No fit_metrics_aggregation_fn provided\n",
      "DEBUG flwr 2023-02-27 15:48:54,519 | server.py:165 | evaluate_round 1: strategy sampled 3 clients (out of 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=15345)\u001b[0m Epoch 1: train loss 0.06501290202140808, accuracy 0.232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-27 15:48:58,422 E 15232 15232] (raylet) node_manager.cc:3097: 6 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 6af4d8029283a981137c11393934d1bd5fe3e6cdbe5cd3d6bfb447e9, IP: 10.246.68.42) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.246.68.42`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=15347)\u001b[0m [Client 2] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=15347)\u001b[0m [Client 8] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=15347)\u001b[0m [Client 5] evaluate, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-02-27 15:49:11,505 | server.py:179 | evaluate_round 1 received 3 results and 0 failures\n",
      "WARNING flwr 2023-02-27 15:49:11,507 | fedavg.py:273 | No evaluate_metrics_aggregation_fn provided\n",
      "DEBUG flwr 2023-02-27 15:49:11,509 | server.py:215 | fit_round 2: strategy sampled 3 clients (out of 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=15351)\u001b[0m [Client 5] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=15351)\u001b[0m Epoch 1: train loss 0.05815477296710014, accuracy 0.316\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=15351)\u001b[0m [Client 8] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=15351)\u001b[0m Epoch 1: train loss 0.05778171867132187, accuracy 0.32066666666666666\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=15351)\u001b[0m [Client 9] fit, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-02-27 15:49:42,322 | server.py:229 | fit_round 2 received 3 results and 0 failures\n",
      "DEBUG flwr 2023-02-27 15:49:42,339 | server.py:165 | evaluate_round 2: strategy sampled 3 clients (out of 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=15351)\u001b[0m Epoch 1: train loss 0.058538973331451416, accuracy 0.31955555555555554\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=15346)\u001b[0m [Client 0] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=15346)\u001b[0m [Client 1] evaluate, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m E0227 15:49:55.026685400   16404 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {\"created\":\"@1677509395.026657100\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":202,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "DEBUG flwr 2023-02-27 15:49:55,189 | server.py:179 | evaluate_round 2 received 3 results and 0 failures\n",
      "DEBUG flwr 2023-02-27 15:49:55,190 | server.py:215 | fit_round 3: strategy sampled 3 clients (out of 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=15346)\u001b[0m [Client 7] evaluate, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-27 15:49:58,423 E 15232 15232] (raylet) node_manager.cc:3097: 2 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 6af4d8029283a981137c11393934d1bd5fe3e6cdbe5cd3d6bfb447e9, IP: 10.246.68.42) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.246.68.42`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=15346)\u001b[0m [Client 8] fit, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(raylet)\u001b[0m Spilled 2206 MiB, 28 objects, write throughput 122 MiB/s. Set RAY_verbose_spill_logs=0 to disable this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=15346)\u001b[0m Epoch 1: train loss 0.05325465276837349, accuracy 0.3804444444444444\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=15346)\u001b[0m [Client 0] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=15346)\u001b[0m Epoch 1: train loss 0.05432221665978432, accuracy 0.3668888888888889\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=15346)\u001b[0m [Client 3] fit, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-02-27 15:50:11,544 | server.py:229 | fit_round 3 received 3 results and 0 failures\n",
      "DEBUG flwr 2023-02-27 15:50:11,564 | server.py:165 | evaluate_round 3: strategy sampled 3 clients (out of 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=15346)\u001b[0m Epoch 1: train loss 0.05534721538424492, accuracy 0.3526666666666667\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=15346)\u001b[0m [Client 2] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=15346)\u001b[0m [Client 1] evaluate, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-02-27 15:50:21,974 | server.py:179 | evaluate_round 3 received 3 results and 0 failures\n",
      "INFO flwr 2023-02-27 15:50:21,975 | server.py:144 | FL finished in 132.40289959999973\n",
      "INFO flwr 2023-02-27 15:50:21,977 | app.py:202 | app_fit: losses_distributed [(1, 0.06310507273674011), (2, 0.05618654195467632), (3, 0.05283888673782348)]\n",
      "INFO flwr 2023-02-27 15:50:21,978 | app.py:203 | app_fit: metrics_distributed {}\n",
      "INFO flwr 2023-02-27 15:50:21,979 | app.py:204 | app_fit: losses_centralized []\n",
      "INFO flwr 2023-02-27 15:50:21,981 | app.py:205 | app_fit: metrics_centralized {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=15346)\u001b[0m [Client 5] evaluate, config: {}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "History (loss, distributed):\n",
       "\tround 1: 0.06310507273674011\n",
       "\tround 2: 0.05618654195467632\n",
       "\tround 3: 0.05283888673782348"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of the model and get the parameters\n",
    "params = get_parameters(Net())\n",
    "\n",
    "# Pass parameters to the Strategy for server-side parameter initialization\n",
    "strategy = fl.server.strategy.FedAvg(\n",
    "    fraction_fit=0.3,\n",
    "    fraction_evaluate=0.3,\n",
    "    min_fit_clients=3,\n",
    "    min_evaluate_clients=3,\n",
    "    min_available_clients=NUM_CLIENTS,\n",
    "    initial_parameters=fl.common.ndarrays_to_parameters(params),\n",
    ")\n",
    "\n",
    "# Specify client resources if you need GPU (defaults to 1 CPU and 0 GPU)\n",
    "client_resources = None\n",
    "if DEVICE.type == \"cuda\":\n",
    "    client_resources = {\"num_gpus\": 1}\n",
    "\n",
    "# Start simulation\n",
    "fl.simulation.start_simulation(\n",
    "    client_fn=client_fn,\n",
    "    num_clients=NUM_CLIENTS,\n",
    "    config=fl.server.ServerConfig(num_rounds=3),  # Just three rounds\n",
    "    strategy=strategy,\n",
    "    client_resources=client_resources,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing `initial_parameters` to the `FedAvg` strategy prevents Flower from asking one of the clients for the initial parameters. If we look closely, we can see that the logs do not show any calls to the `FlowerClient.get_parameters` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting with a customized strategy\n",
    "\n",
    "We've seen the function `start_simulation` before. It accepts a number of arguments, amongst them the `client_fn` used to create `FlowerClient` instances, the number of clients to simulate `num_clients`, the number of rounds `num_rounds`, and the strategy.\n",
    "\n",
    "The strategy encapsulates the federated learning approach/algorithm, for example, `FedAvg` or `FedAdagrad`. Let's try to use a different strategy this time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-02-27 15:50:22,257 | app.py:145 | Starting Flower simulation, config: ServerConfig(num_rounds=3, round_timeout=None)\n",
      "2023-02-27 15:50:37,970\tINFO worker.py:1529 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "INFO flwr 2023-02-27 15:50:44,312 | app.py:179 | Flower VCE: Ray initialized with resources: {'object_store_memory': 1141410201.0, 'memory': 2282820404.0, 'CPU': 8.0, 'node:10.246.68.42': 1.0}\n",
      "INFO flwr 2023-02-27 15:50:44,315 | server.py:86 | Initializing global parameters\n",
      "INFO flwr 2023-02-27 15:50:44,317 | server.py:266 | Using initial parameters provided by strategy\n",
      "INFO flwr 2023-02-27 15:50:44,320 | server.py:88 | Evaluating initial parameters\n",
      "INFO flwr 2023-02-27 15:50:44,324 | server.py:101 | FL starting\n",
      "DEBUG flwr 2023-02-27 15:50:44,327 | server.py:215 | fit_round 1: strategy sampled 3 clients (out of 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=16961)\u001b[0m [Client 1] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=16963)\u001b[0m [Client 9] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=16962)\u001b[0m [Client 8] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=16963)\u001b[0m Epoch 1: train loss 0.06593215465545654, accuracy 0.22088888888888888\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=16961)\u001b[0m Epoch 1: train loss 0.06524761766195297, accuracy 0.2222222222222222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-02-27 15:51:09,997 | server.py:229 | fit_round 1 received 3 results and 0 failures\n",
      "WARNING flwr 2023-02-27 15:51:10,028 | fedavg.py:242 | No fit_metrics_aggregation_fn provided\n",
      "DEBUG flwr 2023-02-27 15:51:10,039 | server.py:165 | evaluate_round 1: strategy sampled 3 clients (out of 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=16962)\u001b[0m Epoch 1: train loss 0.06476739794015884, accuracy 0.21288888888888888\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=16961)\u001b[0m [Client 1] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=16963)\u001b[0m [Client 6] evaluate, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m E0227 15:51:28.906780400   17492 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {\"created\":\"@1677509488.906747200\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":202,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m E0227 15:51:28.942513900   17507 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {\"created\":\"@1677509488.942477300\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":202,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m E0227 15:51:30.526282900   17508 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {\"created\":\"@1677509490.526099000\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":202,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=16945)\u001b[0m [Client 7] evaluate, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-02-27 15:51:32,948 | server.py:179 | evaluate_round 1 received 3 results and 0 failures\n",
      "WARNING flwr 2023-02-27 15:51:32,950 | fedavg.py:273 | No evaluate_metrics_aggregation_fn provided\n",
      "DEBUG flwr 2023-02-27 15:51:32,953 | server.py:215 | fit_round 2: strategy sampled 3 clients (out of 10)\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-27 15:51:37,883 E 16823 16823] (raylet) node_manager.cc:3097: 2 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 8e76133d0765063e4f71166921b50bf0c8c576e43bf796615c9beca4, IP: 10.246.68.42) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.246.68.42`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=16963)\u001b[0m [Client 7] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=16961)\u001b[0m [Client 6] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=16961)\u001b[0m Epoch 1: train loss 0.6562023162841797, accuracy 0.2782222222222222\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=16963)\u001b[0m Epoch 1: train loss 0.6462485790252686, accuracy 0.2942222222222222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m E0227 15:51:50.488486200   17606 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {\"created\":\"@1677509510.488448400\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":202,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=16959)\u001b[0m [Client 0] fit, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-02-27 15:52:07,655 | server.py:229 | fit_round 2 received 3 results and 0 failures\n",
      "DEBUG flwr 2023-02-27 15:52:07,690 | server.py:165 | evaluate_round 2: strategy sampled 3 clients (out of 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=16959)\u001b[0m Epoch 1: train loss 0.6790484189987183, accuracy 0.29844444444444446\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=16961)\u001b[0m [Client 5] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=16963)\u001b[0m [Client 0] evaluate, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-02-27 15:52:13,458 | server.py:179 | evaluate_round 2 received 3 results and 0 failures\n",
      "DEBUG flwr 2023-02-27 15:52:13,459 | server.py:215 | fit_round 3: strategy sampled 3 clients (out of 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=16959)\u001b[0m [Client 1] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=16961)\u001b[0m [Client 7] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=16959)\u001b[0m [Client 0] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=16963)\u001b[0m [Client 1] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=16961)\u001b[0m Epoch 1: train loss 0.08753269910812378, accuracy 0.17644444444444443\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=16959)\u001b[0m Epoch 1: train loss 0.09109263867139816, accuracy 0.17177777777777778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-02-27 15:52:24,003 | server.py:229 | fit_round 3 received 3 results and 0 failures\n",
      "DEBUG flwr 2023-02-27 15:52:24,021 | server.py:165 | evaluate_round 3: strategy sampled 3 clients (out of 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=16963)\u001b[0m Epoch 1: train loss 0.08876537531614304, accuracy 0.184\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=16963)\u001b[0m [Client 8] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=16961)\u001b[0m [Client 9] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=16959)\u001b[0m [Client 1] evaluate, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(raylet)\u001b[0m Spilled 2058 MiB, 25 objects, write throughput 118 MiB/s. Set RAY_verbose_spill_logs=0 to disable this message.\n",
      "DEBUG flwr 2023-02-27 15:52:30,545 | server.py:179 | evaluate_round 3 received 3 results and 0 failures\n",
      "INFO flwr 2023-02-27 15:52:30,546 | server.py:144 | FL finished in 106.2191026999999\n",
      "INFO flwr 2023-02-27 15:52:30,547 | app.py:202 | app_fit: losses_distributed [(1, 6.097165802001953), (2, 0.43599525769551595), (3, 0.14549784088134768)]\n",
      "INFO flwr 2023-02-27 15:52:30,549 | app.py:203 | app_fit: metrics_distributed {}\n",
      "INFO flwr 2023-02-27 15:52:30,550 | app.py:204 | app_fit: losses_centralized []\n",
      "INFO flwr 2023-02-27 15:52:30,551 | app.py:205 | app_fit: metrics_centralized {}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "History (loss, distributed):\n",
       "\tround 1: 6.097165802001953\n",
       "\tround 2: 0.43599525769551595\n",
       "\tround 3: 0.14549784088134768"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create FedAdam strategy\n",
    "strategy = fl.server.strategy.FedAdagrad(\n",
    "    fraction_fit=0.3,\n",
    "    fraction_evaluate=0.3,\n",
    "    min_fit_clients=3,\n",
    "    min_evaluate_clients=3,\n",
    "    min_available_clients=NUM_CLIENTS,\n",
    "    initial_parameters=fl.common.ndarrays_to_parameters(get_parameters(Net())),\n",
    ")\n",
    "\n",
    "# Start simulation\n",
    "fl.simulation.start_simulation(\n",
    "    client_fn=client_fn,\n",
    "    num_clients=NUM_CLIENTS,\n",
    "    config=fl.server.ServerConfig(num_rounds=3),  # Just three rounds\n",
    "    strategy=strategy,\n",
    "    client_resources=client_resources,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Server-side parameter **evaluation**\n",
    "\n",
    "Flower can evaluate the aggregated model on the server-side or on the client-side. Client-side and server-side evaluation are similar in some ways, but different in others.\n",
    "\n",
    "**Centralized Evaluation** (or *server-side evaluation*) is conceptually simple: it works the same way that evaluation in centralized machine learning does. If there is a server-side dataset that can be used for evaluation purposes, then that's great. We can evaluate the newly aggregated model after each round of training without having to send the model to clients. We're also fortunate in the sense that our entire evaluation dataset is available at all times.\n",
    "\n",
    "**Federated Evaluation** (or *client-side evaluation*) is more complex, but also more powerful: it doesn't require a centralized dataset and allows us to evaluate models over a larger set of data, which often yields more realistic evaluation results. In fact, many scenarios require us to use **Federated Evaluation** if we want to get representative evaluation results at all. But this power comes at a cost: once we start to evaluate on the client side, we should be aware that our evaluation dataset can change over consecutive rounds of learning if those clients are not always available. Moreover, the dataset held by each client can also change over consecutive rounds. This can lead to evaluation results that are not stable, so even if we would not change the model, we'd see our evaluation results fluctuate over consecutive rounds.\n",
    "\n",
    "We've seen how federated evaluation works on the client side (i.e., by implementing the `evaluate` method in `FlowerClient`). Now let's see how we can evaluate aggregated model parameters on the server-side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `evaluate` function will be by Flower called after every round\n",
    "def evaluate(\n",
    "    server_round: int,\n",
    "    parameters: fl.common.NDArrays,\n",
    "    config: Dict[str, fl.common.Scalar],\n",
    ") -> Optional[Tuple[float, Dict[str, fl.common.Scalar]]]:\n",
    "    net = Net().to(DEVICE)\n",
    "    valloader = valloaders[0]\n",
    "    set_parameters(net, parameters)  # Update model with the latest parameters\n",
    "    loss, accuracy = test(net, valloader)\n",
    "    print(f\"Server-side evaluation loss {loss} / accuracy {accuracy}\")\n",
    "    return loss, {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-02-27 15:52:30,898 | app.py:145 | Starting Flower simulation, config: ServerConfig(num_rounds=3, round_timeout=None)\n",
      "2023-02-27 15:52:48,259\tINFO worker.py:1529 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "INFO flwr 2023-02-27 15:52:54,731 | app.py:179 | Flower VCE: Ray initialized with resources: {'memory': 3021282510.0, 'node:10.246.68.42': 1.0, 'CPU': 8.0, 'object_store_memory': 1510641254.0}\n",
      "INFO flwr 2023-02-27 15:52:54,742 | server.py:86 | Initializing global parameters\n",
      "INFO flwr 2023-02-27 15:52:54,747 | server.py:266 | Using initial parameters provided by strategy\n",
      "INFO flwr 2023-02-27 15:52:54,752 | server.py:88 | Evaluating initial parameters\n",
      "INFO flwr 2023-02-27 15:52:55,361 | server.py:91 | initial parameters (loss, other metrics): 0.07379200744628907, {'accuracy': 0.096}\n",
      "INFO flwr 2023-02-27 15:52:55,364 | server.py:101 | FL starting\n",
      "DEBUG flwr 2023-02-27 15:52:55,375 | server.py:215 | fit_round 1: strategy sampled 3 clients (out of 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server-side evaluation loss 0.07379200744628907 / accuracy 0.096\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=18133)\u001b[0m [Client 8] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=18134)\u001b[0m [Client 0] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=18135)\u001b[0m [Client 2] fit, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-02-27 15:53:12,852 | server.py:229 | fit_round 1 received 3 results and 0 failures\n",
      "WARNING flwr 2023-02-27 15:53:12,862 | fedavg.py:242 | No fit_metrics_aggregation_fn provided\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=18133)\u001b[0m Epoch 1: train loss 0.06419975310564041, accuracy 0.23044444444444445\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=18134)\u001b[0m Epoch 1: train loss 0.06415960192680359, accuracy 0.24155555555555555\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=18135)\u001b[0m Epoch 1: train loss 0.06351277977228165, accuracy 0.23644444444444446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-02-27 15:53:13,068 | server.py:116 | fit progress: (1, 0.0616036479473114, {'accuracy': 0.31}, 17.692429400000037)\n",
      "DEBUG flwr 2023-02-27 15:53:13,069 | server.py:165 | evaluate_round 1: strategy sampled 3 clients (out of 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server-side evaluation loss 0.0616036479473114 / accuracy 0.31\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=18135)\u001b[0m [Client 1] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=18133)\u001b[0m [Client 2] evaluate, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-02-27 15:53:19,531 | server.py:179 | evaluate_round 1 received 3 results and 0 failures\n",
      "WARNING flwr 2023-02-27 15:53:19,532 | fedavg.py:273 | No evaluate_metrics_aggregation_fn provided\n",
      "DEBUG flwr 2023-02-27 15:53:19,533 | server.py:215 | fit_round 2: strategy sampled 3 clients (out of 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=18134)\u001b[0m [Client 0] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=18133)\u001b[0m [Client 1] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=18134)\u001b[0m [Client 0] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=18135)\u001b[0m [Client 6] fit, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-02-27 15:53:28,714 | server.py:229 | fit_round 2 received 3 results and 0 failures\n",
      "INFO flwr 2023-02-27 15:53:28,909 | server.py:116 | fit progress: (2, 0.053530490636825565, {'accuracy': 0.376}, 33.53404150000006)\n",
      "DEBUG flwr 2023-02-27 15:53:28,910 | server.py:165 | evaluate_round 2: strategy sampled 3 clients (out of 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=18133)\u001b[0m Epoch 1: train loss 0.05504786968231201, accuracy 0.35644444444444445\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=18134)\u001b[0m Epoch 1: train loss 0.05558769404888153, accuracy 0.3453333333333333\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=18135)\u001b[0m Epoch 1: train loss 0.05689386650919914, accuracy 0.33155555555555555\n",
      "Server-side evaluation loss 0.053530490636825565 / accuracy 0.376\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=18133)\u001b[0m [Client 7] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=18134)\u001b[0m [Client 8] evaluate, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m E0227 15:53:38.473243800   18512 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {\"created\":\"@1677509618.473205700\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":202,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m E0227 15:53:38.490064700   18511 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {\"created\":\"@1677509618.490030900\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":202,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m E0227 15:53:38.742873000   18507 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {\"created\":\"@1677509618.742844600\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":202,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=18130)\u001b[0m [Client 2] evaluate, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-02-27 15:53:39,952 | server.py:179 | evaluate_round 2 received 3 results and 0 failures\n",
      "DEBUG flwr 2023-02-27 15:53:39,954 | server.py:215 | fit_round 3: strategy sampled 3 clients (out of 10)\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m E0227 15:53:40.467910800   18600 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {\"created\":\"@1677509620.467884100\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":202,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=18133)\u001b[0m [Client 2] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=18134)\u001b[0m [Client 4] fit, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=18130)\u001b[0m [Client 1] fit, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-27 15:53:48,188 E 18038 18038] (raylet) node_manager.cc:3097: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: e733e0509a7835e3c6781a480005aa97080ba577eefd8322d41c1712, IP: 10.246.68.42) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.246.68.42`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "DEBUG flwr 2023-02-27 15:53:50,416 | server.py:229 | fit_round 3 received 3 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=18134)\u001b[0m Epoch 1: train loss 0.05138608440756798, accuracy 0.38755555555555554\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=18133)\u001b[0m Epoch 1: train loss 0.05228734388947487, accuracy 0.39844444444444443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-02-27 15:53:50,612 | server.py:116 | fit progress: (3, 0.05148048067092895, {'accuracy': 0.428}, 55.23696760000075)\n",
      "DEBUG flwr 2023-02-27 15:53:50,613 | server.py:165 | evaluate_round 3: strategy sampled 3 clients (out of 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=18130)\u001b[0m Epoch 1: train loss 0.05163895711302757, accuracy 0.3933333333333333\n",
      "Server-side evaluation loss 0.05148048067092895 / accuracy 0.428\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=18130)\u001b[0m [Client 8] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=18134)\u001b[0m [Client 2] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=18132)\u001b[0m [Client 1] evaluate, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-02-27 15:54:01,253 | server.py:179 | evaluate_round 3 received 3 results and 0 failures\n",
      "INFO flwr 2023-02-27 15:54:01,255 | server.py:144 | FL finished in 65.87967420000041\n",
      "INFO flwr 2023-02-27 15:54:01,256 | app.py:202 | app_fit: losses_distributed [(1, 0.06166497111320496), (2, 0.05276294088363647), (3, 0.05091049361228942)]\n",
      "INFO flwr 2023-02-27 15:54:01,257 | app.py:203 | app_fit: metrics_distributed {}\n",
      "INFO flwr 2023-02-27 15:54:01,258 | app.py:204 | app_fit: losses_centralized [(0, 0.07379200744628907), (1, 0.0616036479473114), (2, 0.053530490636825565), (3, 0.05148048067092895)]\n",
      "INFO flwr 2023-02-27 15:54:01,259 | app.py:205 | app_fit: metrics_centralized {'accuracy': [(0, 0.096), (1, 0.31), (2, 0.376), (3, 0.428)]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "History (loss, distributed):\n",
       "\tround 1: 0.06166497111320496\n",
       "\tround 2: 0.05276294088363647\n",
       "\tround 3: 0.05091049361228942\n",
       "History (loss, centralized):\n",
       "\tround 0: 0.07379200744628907\n",
       "\tround 1: 0.0616036479473114\n",
       "\tround 2: 0.053530490636825565\n",
       "\tround 3: 0.05148048067092895\n",
       "History (metrics, centralized):\n",
       "{'accuracy': [(0, 0.096), (1, 0.31), (2, 0.376), (3, 0.428)]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strategy = fl.server.strategy.FedAvg(\n",
    "    fraction_fit=0.3,\n",
    "    fraction_evaluate=0.3,\n",
    "    min_fit_clients=3,\n",
    "    min_evaluate_clients=3,\n",
    "    min_available_clients=NUM_CLIENTS,\n",
    "    initial_parameters=fl.common.ndarrays_to_parameters(get_parameters(Net())),\n",
    "    evaluate_fn=evaluate,  # Pass the evaluation function\n",
    ")\n",
    "\n",
    "fl.simulation.start_simulation(\n",
    "    client_fn=client_fn,\n",
    "    num_clients=NUM_CLIENTS,\n",
    "    config=fl.server.ServerConfig(num_rounds=3),  # Just three rounds\n",
    "    strategy=strategy,\n",
    "    client_resources=client_resources,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending/receiving arbitrary values to/from clients\n",
    "\n",
    "In some situations, we want to configure client-side execution (trainig, evaluation) from the server-side. One example for that is the server asking the clients to train for a certain number of local epochs. Flower provides a way to send configuration values from the server to the clients using a dictionary. Let's look at an example where the clients receive values from the server through the `config` parameter in `fit` (`config` is also available in `evaluate`). The `fit` method receives the configuration dictionary through the `config` parameter and can then read values from this dictionary. In this example, it reads `server_round` and `local_epochs` and uses those values to improve the logging and configure the number of local training epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowerClient(fl.client.NumPyClient):\n",
    "    def __init__(self, cid, net, trainloader, valloader):\n",
    "        self.cid = cid\n",
    "        self.net = net\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = valloader\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        print(f\"[Client {self.cid}] get_parameters\")\n",
    "        return get_parameters(self.net)\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        # Read values from config\n",
    "        server_round = config[\"server_round\"]\n",
    "        local_epochs = config[\"local_epochs\"]\n",
    "\n",
    "        # Use values provided by the config\n",
    "        print(f\"[Client {self.cid}, round {server_round}] fit, config: {config}\")\n",
    "        set_parameters(self.net, parameters)\n",
    "        train(self.net, self.trainloader, epochs=local_epochs)\n",
    "        return get_parameters(self.net), len(self.trainloader), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        print(f\"[Client {self.cid}] evaluate, config: {config}\")\n",
    "        set_parameters(self.net, parameters)\n",
    "        loss, accuracy = test(self.net, self.valloader)\n",
    "        return float(loss), len(self.valloader), {\"accuracy\": float(accuracy)}\n",
    "\n",
    "\n",
    "def client_fn(cid) -> FlowerClient:\n",
    "    net = Net().to(DEVICE)\n",
    "    trainloader = trainloaders[int(cid)]\n",
    "    valloader = valloaders[int(cid)]\n",
    "    return FlowerClient(cid, net, trainloader, valloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how can we  send this config dictionary from server to clients? The built-in Flower Strategies provide way to do this, and it works similarly to the way server-side evaluation works. We provide a function to the strategy, and the strategy calls this function for every round of federated learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_config(server_round: int):\n",
    "    \"\"\"Return training configuration dict for each round.\n",
    "\n",
    "    Perform two rounds of training with one local epoch, increase to two local\n",
    "    epochs afterwards.\n",
    "    \"\"\"\n",
    "    config = {\n",
    "        \"server_round\": server_round,  # The current round of federated learning\n",
    "        \"local_epochs\": 1 if server_round < 2 else 2,  #\n",
    "    }\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll just pass this function to the FedAvg strategy before starting the simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-02-27 15:54:01,832 | app.py:145 | Starting Flower simulation, config: ServerConfig(num_rounds=3, round_timeout=None)\n",
      "2023-02-27 15:54:12,711\tINFO worker.py:1529 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "INFO flwr 2023-02-27 15:54:17,699 | app.py:179 | Flower VCE: Ray initialized with resources: {'node:10.246.68.42': 1.0, 'memory': 3294825678.0, 'object_store_memory': 1647412838.0, 'CPU': 8.0}\n",
      "INFO flwr 2023-02-27 15:54:17,701 | server.py:86 | Initializing global parameters\n",
      "INFO flwr 2023-02-27 15:54:17,702 | server.py:266 | Using initial parameters provided by strategy\n",
      "INFO flwr 2023-02-27 15:54:17,703 | server.py:88 | Evaluating initial parameters\n",
      "INFO flwr 2023-02-27 15:54:17,948 | server.py:91 | initial parameters (loss, other metrics): 0.07383857822418213, {'accuracy': 0.102}\n",
      "INFO flwr 2023-02-27 15:54:17,949 | server.py:101 | FL starting\n",
      "DEBUG flwr 2023-02-27 15:54:17,950 | server.py:215 | fit_round 1: strategy sampled 3 clients (out of 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server-side evaluation loss 0.07383857822418213 / accuracy 0.102\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=18904)\u001b[0m [Client 2, round 1] fit, config: {'server_round': 1, 'local_epochs': 1}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=18908)\u001b[0m [Client 6, round 1] fit, config: {'server_round': 1, 'local_epochs': 1}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=18909)\u001b[0m [Client 1, round 1] fit, config: {'server_round': 1, 'local_epochs': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-02-27 15:54:30,967 | server.py:229 | fit_round 1 received 3 results and 0 failures\n",
      "WARNING flwr 2023-02-27 15:54:30,981 | fedavg.py:242 | No fit_metrics_aggregation_fn provided\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=18904)\u001b[0m Epoch 1: train loss 0.06377512961626053, accuracy 0.24488888888888888\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=18908)\u001b[0m Epoch 1: train loss 0.06503230333328247, accuracy 0.22088888888888888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-02-27 15:54:31,216 | server.py:116 | fit progress: (1, 0.06176053500175476, {'accuracy': 0.304}, 13.26582099999996)\n",
      "DEBUG flwr 2023-02-27 15:54:31,217 | server.py:165 | evaluate_round 1: strategy sampled 3 clients (out of 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=18909)\u001b[0m Epoch 1: train loss 0.06389319151639938, accuracy 0.23955555555555555\n",
      "Server-side evaluation loss 0.06176053500175476 / accuracy 0.304\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=18908)\u001b[0m [Client 4] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=18909)\u001b[0m [Client 8] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=18904)\u001b[0m [Client 3] evaluate, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-02-27 15:54:36,531 | server.py:179 | evaluate_round 1 received 3 results and 0 failures\n",
      "WARNING flwr 2023-02-27 15:54:36,532 | fedavg.py:273 | No evaluate_metrics_aggregation_fn provided\n",
      "DEBUG flwr 2023-02-27 15:54:36,533 | server.py:215 | fit_round 2: strategy sampled 3 clients (out of 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=18904)\u001b[0m [Client 4, round 2] fit, config: {'server_round': 2, 'local_epochs': 2}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=18908)\u001b[0m [Client 7, round 2] fit, config: {'server_round': 2, 'local_epochs': 2}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=18909)\u001b[0m [Client 0, round 2] fit, config: {'server_round': 2, 'local_epochs': 2}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=18904)\u001b[0m Epoch 1: train loss 0.05752306431531906, accuracy 0.318\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=18908)\u001b[0m Epoch 1: train loss 0.05670631676912308, accuracy 0.32822222222222225\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=18909)\u001b[0m Epoch 1: train loss 0.05715209245681763, accuracy 0.33466666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-02-27 15:54:48,500 | server.py:229 | fit_round 2 received 3 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=18904)\u001b[0m Epoch 2: train loss 0.05319346860051155, accuracy 0.37377777777777776\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=18908)\u001b[0m Epoch 2: train loss 0.052408479154109955, accuracy 0.38466666666666666\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=18909)\u001b[0m Epoch 2: train loss 0.053283073008060455, accuracy 0.3811111111111111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-02-27 15:54:48,725 | server.py:116 | fit progress: (2, 0.05338016629219055, {'accuracy': 0.38}, 30.775509900000543)\n",
      "DEBUG flwr 2023-02-27 15:54:48,726 | server.py:165 | evaluate_round 2: strategy sampled 3 clients (out of 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server-side evaluation loss 0.05338016629219055 / accuracy 0.38\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=18904)\u001b[0m [Client 1] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=18908)\u001b[0m [Client 9] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=18909)\u001b[0m [Client 2] evaluate, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-02-27 15:54:56,357 | server.py:179 | evaluate_round 2 received 3 results and 0 failures\n",
      "DEBUG flwr 2023-02-27 15:54:56,362 | server.py:215 | fit_round 3: strategy sampled 3 clients (out of 10)\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m E0227 15:54:57.981341300   19281 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {\"created\":\"@1677509697.981300100\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":202,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m E0227 15:54:58.288603100   19279 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {\"created\":\"@1677509698.288569100\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":202,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m E0227 15:54:58.293286000   19280 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {\"created\":\"@1677509698.293249600\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":202,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m E0227 15:55:00.147563700   19357 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {\"created\":\"@1677509700.147539900\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":202,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=18904)\u001b[0m [Client 2, round 3] fit, config: {'server_round': 3, 'local_epochs': 2}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=18908)\u001b[0m [Client 8, round 3] fit, config: {'server_round': 3, 'local_epochs': 2}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=18909)\u001b[0m [Client 6, round 3] fit, config: {'server_round': 3, 'local_epochs': 2}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=18904)\u001b[0m Epoch 1: train loss 0.0522049255669117, accuracy 0.386\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=18909)\u001b[0m Epoch 1: train loss 0.05285734310746193, accuracy 0.37977777777777777\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=18908)\u001b[0m Epoch 1: train loss 0.05055960267782211, accuracy 0.412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-02-27 15:55:11,958 | server.py:229 | fit_round 3 received 3 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=18904)\u001b[0m Epoch 2: train loss 0.0494501069188118, accuracy 0.4146666666666667\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=18909)\u001b[0m Epoch 2: train loss 0.04971221089363098, accuracy 0.4151111111111111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-02-27 15:55:12,158 | server.py:116 | fit progress: (3, 0.04911686301231384, {'accuracy': 0.45}, 54.208160099999986)\n",
      "DEBUG flwr 2023-02-27 15:55:12,159 | server.py:165 | evaluate_round 3: strategy sampled 3 clients (out of 10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=18908)\u001b[0m Epoch 2: train loss 0.04801885783672333, accuracy 0.44066666666666665\n",
      "Server-side evaluation loss 0.04911686301231384 / accuracy 0.45\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=18904)\u001b[0m [Client 5] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=18908)\u001b[0m [Client 8] evaluate, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-02-27 15:55:20,202 | server.py:179 | evaluate_round 3 received 3 results and 0 failures\n",
      "INFO flwr 2023-02-27 15:55:20,203 | server.py:144 | FL finished in 62.2529013000003\n",
      "INFO flwr 2023-02-27 15:55:20,204 | app.py:202 | app_fit: losses_distributed [(1, 0.06204264259338379), (2, 0.05329331096013387), (3, 0.04731542666753133)]\n",
      "INFO flwr 2023-02-27 15:55:20,204 | app.py:203 | app_fit: metrics_distributed {}\n",
      "INFO flwr 2023-02-27 15:55:20,205 | app.py:204 | app_fit: losses_centralized [(0, 0.07383857822418213), (1, 0.06176053500175476), (2, 0.05338016629219055), (3, 0.04911686301231384)]\n",
      "INFO flwr 2023-02-27 15:55:20,207 | app.py:205 | app_fit: metrics_centralized {'accuracy': [(0, 0.102), (1, 0.304), (2, 0.38), (3, 0.45)]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=18908)\u001b[0m [Client 7] evaluate, config: {}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "History (loss, distributed):\n",
       "\tround 1: 0.06204264259338379\n",
       "\tround 2: 0.05329331096013387\n",
       "\tround 3: 0.04731542666753133\n",
       "History (loss, centralized):\n",
       "\tround 0: 0.07383857822418213\n",
       "\tround 1: 0.06176053500175476\n",
       "\tround 2: 0.05338016629219055\n",
       "\tround 3: 0.04911686301231384\n",
       "History (metrics, centralized):\n",
       "{'accuracy': [(0, 0.102), (1, 0.304), (2, 0.38), (3, 0.45)]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strategy = fl.server.strategy.FedAvg(\n",
    "    fraction_fit=0.3,\n",
    "    fraction_evaluate=0.3,\n",
    "    min_fit_clients=3,\n",
    "    min_evaluate_clients=3,\n",
    "    min_available_clients=NUM_CLIENTS,\n",
    "    initial_parameters=fl.common.ndarrays_to_parameters(get_parameters(Net())),\n",
    "    evaluate_fn=evaluate,\n",
    "    on_fit_config_fn=fit_config,  # Pass the fit_config function\n",
    ")\n",
    "\n",
    "fl.simulation.start_simulation(\n",
    "    client_fn=client_fn,\n",
    "    num_clients=NUM_CLIENTS,\n",
    "    config=fl.server.ServerConfig(num_rounds=3),  # Just three rounds\n",
    "    strategy=strategy,\n",
    "    client_resources=client_resources,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the client logs now include the current round of federated learning (which they read from the `config` dictionary). We can also configure local training to run for one epoch during the first and second round of federated learning, and then for two epochs during the third round.\n",
    "\n",
    "Clients can also return arbitrary values to the server. To do so, they return a dictionary from `fit` and/or `evaluate`. We have seen and used this concept throughout this notebook without mentioning it explicitly: our `FlowerClient` returns a dictionary containing a custom key/value pair as the third return value in `evaluate`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling federated learning\n",
    "\n",
    "As a last step in this notebook, let's see how we can use Flower to experiment with a large number of clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "NUM_CLIENTS = 1000\n",
    "\n",
    "trainloaders, valloaders, testloader = load_datasets(NUM_CLIENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 1000 partitions, each holding 45 training and 5 validation examples. Given that the number of training examples on each client is quite small, we should probably train the model a bit longer, so we configure the clients to perform 3 local training epochs. We should also adjust the fraction of clients selected for training during each round (we don't want all 1000 clients participating in every round), so we adjust `fraction_fit` to `0.05`, which means that only 5% of available clients (so 50 clients) will be selected for training each round:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-02-27 15:55:24,654 | app.py:145 | Starting Flower simulation, config: ServerConfig(num_rounds=3, round_timeout=None)\n",
      "2023-02-27 15:55:36,960\tINFO worker.py:1529 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "INFO flwr 2023-02-27 15:55:41,067 | app.py:179 | Flower VCE: Ray initialized with resources: {'object_store_memory': 1663985664.0, 'CPU': 8.0, 'memory': 3327971328.0, 'node:10.246.68.42': 1.0}\n",
      "INFO flwr 2023-02-27 15:55:41,077 | server.py:86 | Initializing global parameters\n",
      "INFO flwr 2023-02-27 15:55:41,078 | server.py:266 | Using initial parameters provided by strategy\n",
      "INFO flwr 2023-02-27 15:55:41,080 | server.py:88 | Evaluating initial parameters\n",
      "INFO flwr 2023-02-27 15:55:41,081 | server.py:101 | FL starting\n",
      "DEBUG flwr 2023-02-27 15:55:41,083 | server.py:215 | fit_round 1: strategy sampled 25 clients (out of 1000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=19674)\u001b[0m [Client 823, round 1] fit, config: {'server_round': 1, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19674)\u001b[0m Epoch 1: train loss 0.10284201055765152, accuracy 0.06666666666666667\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19674)\u001b[0m Epoch 2: train loss 0.10172911733388901, accuracy 0.1111111111111111\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19674)\u001b[0m Epoch 3: train loss 0.10133899748325348, accuracy 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m E0227 15:56:06.904156700   20025 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {\"created\":\"@1677509766.904120400\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":202,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m E0227 15:56:06.934880100   20026 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {\"created\":\"@1677509766.934844000\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":202,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m E0227 15:56:06.966421600   20021 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {\"created\":\"@1677509766.966383100\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":202,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m E0227 15:56:06.989587700   20027 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {\"created\":\"@1677509766.989542600\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":202,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=19675)\u001b[0m [Client 724, round 1] fit, config: {'server_round': 1, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19675)\u001b[0m Epoch 1: train loss 0.10198123753070831, accuracy 0.08888888888888889\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19672)\u001b[0m [Client 498, round 1] fit, config: {'server_round': 1, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19673)\u001b[0m [Client 962, round 1] fit, config: {'server_round': 1, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m [Client 47, round 1] fit, config: {'server_round': 1, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19675)\u001b[0m Epoch 2: train loss 0.1017126739025116, accuracy 0.17777777777777778\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19675)\u001b[0m Epoch 3: train loss 0.10045036673545837, accuracy 0.2222222222222222\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19672)\u001b[0m Epoch 1: train loss 0.10291380435228348, accuracy 0.06666666666666667\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19673)\u001b[0m Epoch 1: train loss 0.1025865226984024, accuracy 0.13333333333333333\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m Epoch 1: train loss 0.103032685816288, accuracy 0.06666666666666667\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m Epoch 2: train loss 0.10182356089353561, accuracy 0.26666666666666666\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19672)\u001b[0m Epoch 2: train loss 0.10200871527194977, accuracy 0.15555555555555556\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19672)\u001b[0m Epoch 3: train loss 0.10125694423913956, accuracy 0.35555555555555557\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19673)\u001b[0m Epoch 2: train loss 0.10197596997022629, accuracy 0.13333333333333333\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19673)\u001b[0m Epoch 3: train loss 0.10126972943544388, accuracy 0.17777777777777778\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m Epoch 3: train loss 0.0998733714222908, accuracy 0.3111111111111111\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19673)\u001b[0m [Client 748, round 1] fit, config: {'server_round': 1, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19673)\u001b[0m Epoch 1: train loss 0.102405846118927, accuracy 0.044444444444444446\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19672)\u001b[0m [Client 262, round 1] fit, config: {'server_round': 1, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19673)\u001b[0m Epoch 2: train loss 0.10135968774557114, accuracy 0.28888888888888886\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19673)\u001b[0m Epoch 3: train loss 0.10061018168926239, accuracy 0.26666666666666666\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19672)\u001b[0m Epoch 1: train loss 0.10160629451274872, accuracy 0.13333333333333333\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19672)\u001b[0m Epoch 2: train loss 0.10170387476682663, accuracy 0.15555555555555556\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19672)\u001b[0m Epoch 3: train loss 0.10114775598049164, accuracy 0.28888888888888886\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m [Client 8, round 1] fit, config: {'server_round': 1, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19675)\u001b[0m [Client 955, round 1] fit, config: {'server_round': 1, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m Epoch 1: train loss 0.10282499343156815, accuracy 0.08888888888888889\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m Epoch 2: train loss 0.10221648961305618, accuracy 0.06666666666666667\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19675)\u001b[0m Epoch 1: train loss 0.10253863036632538, accuracy 0.15555555555555556\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19675)\u001b[0m Epoch 2: train loss 0.10192826390266418, accuracy 0.1111111111111111\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m Epoch 3: train loss 0.10127205401659012, accuracy 0.2\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19675)\u001b[0m Epoch 3: train loss 0.10118947923183441, accuracy 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m E0227 15:56:19.929306200   20184 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {\"created\":\"@1677509779.929271400\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":202,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m E0227 15:56:19.938908800   20182 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {\"created\":\"@1677509779.938870700\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":202,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m E0227 15:56:19.965374500   20183 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {\"created\":\"@1677509779.965342500\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":202,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m E0227 15:56:20.013316600   20180 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {\"created\":\"@1677509780.013291900\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":202,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=19675)\u001b[0m [Client 144, round 1] fit, config: {'server_round': 1, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19675)\u001b[0m Epoch 1: train loss 0.10193154960870743, accuracy 0.13333333333333333\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19675)\u001b[0m Epoch 2: train loss 0.10134268552064896, accuracy 0.2\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19675)\u001b[0m Epoch 3: train loss 0.1008644551038742, accuracy 0.2\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m [Client 742, round 1] fit, config: {'server_round': 1, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m Epoch 1: train loss 0.1021660789847374, accuracy 0.15555555555555556\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m Epoch 2: train loss 0.10189219564199448, accuracy 0.13333333333333333\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m Epoch 3: train loss 0.1014726310968399, accuracy 0.13333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-27 15:56:37,550 E 19577 19577] (raylet) node_manager.cc:3097: 3 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 23822b6a08ac13c06b82c01db60956472938564608a9004253685752, IP: 10.246.68.42) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.246.68.42`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=19675)\u001b[0m [Client 137, round 1] fit, config: {'server_round': 1, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19675)\u001b[0m Epoch 1: train loss 0.1023029088973999, accuracy 0.13333333333333333\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19675)\u001b[0m Epoch 2: train loss 0.10124348849058151, accuracy 0.15555555555555556\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19675)\u001b[0m Epoch 3: train loss 0.10075262933969498, accuracy 0.24444444444444444\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m [Client 819, round 1] fit, config: {'server_round': 1, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m Epoch 1: train loss 0.10252483934164047, accuracy 0.13333333333333333\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m Epoch 2: train loss 0.10155557096004486, accuracy 0.17777777777777778\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m Epoch 3: train loss 0.10141512751579285, accuracy 0.2222222222222222\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19671)\u001b[0m [Client 449, round 1] fit, config: {'server_round': 1, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19670)\u001b[0m [Client 623, round 1] fit, config: {'server_round': 1, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19669)\u001b[0m [Client 493, round 1] fit, config: {'server_round': 1, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19671)\u001b[0m Epoch 1: train loss 0.10268113762140274, accuracy 0.06666666666666667\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19670)\u001b[0m Epoch 1: train loss 0.10253306478261948, accuracy 0.1111111111111111\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19670)\u001b[0m Epoch 2: train loss 0.10173863172531128, accuracy 0.13333333333333333\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19669)\u001b[0m Epoch 1: train loss 0.10280147194862366, accuracy 0.1111111111111111\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19669)\u001b[0m Epoch 2: train loss 0.10193116217851639, accuracy 0.1111111111111111\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19671)\u001b[0m Epoch 2: train loss 0.10167741030454636, accuracy 0.13333333333333333\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19671)\u001b[0m Epoch 3: train loss 0.10066767781972885, accuracy 0.2\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19670)\u001b[0m Epoch 3: train loss 0.10116051882505417, accuracy 0.17777777777777778\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19669)\u001b[0m Epoch 3: train loss 0.10148885846138, accuracy 0.2\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20327)\u001b[0m [Client 583, round 1] fit, config: {'server_round': 1, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20327)\u001b[0m Epoch 1: train loss 0.10232298076152802, accuracy 0.15555555555555556\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20327)\u001b[0m Epoch 2: train loss 0.101114422082901, accuracy 0.2222222222222222\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20327)\u001b[0m Epoch 3: train loss 0.1003524586558342, accuracy 0.2222222222222222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(raylet)\u001b[0m Spilled 2654 MiB, 34 objects, write throughput 121 MiB/s. Set RAY_verbose_spill_logs=0 to disable this message.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m E0227 15:56:43.272918400   20321 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {\"created\":\"@1677509803.272881200\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":202,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m E0227 15:56:43.297888400   20327 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {\"created\":\"@1677509803.297849800\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":202,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m E0227 15:56:43.312528300   20329 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {\"created\":\"@1677509803.312488100\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":202,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=20327)\u001b[0m [Client 401, round 1] fit, config: {'server_round': 1, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20327)\u001b[0m Epoch 1: train loss 0.1026434376835823, accuracy 0.08888888888888889\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20327)\u001b[0m Epoch 2: train loss 0.10209785401821136, accuracy 0.17777777777777778\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19671)\u001b[0m [Client 446, round 1] fit, config: {'server_round': 1, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20327)\u001b[0m Epoch 3: train loss 0.10097341239452362, accuracy 0.2\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19671)\u001b[0m Epoch 1: train loss 0.1026061400771141, accuracy 0.08888888888888889\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19671)\u001b[0m Epoch 2: train loss 0.10198001563549042, accuracy 0.17777777777777778\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19671)\u001b[0m Epoch 3: train loss 0.10168256610631943, accuracy 0.17777777777777778\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19670)\u001b[0m [Client 730, round 1] fit, config: {'server_round': 1, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19670)\u001b[0m Epoch 1: train loss 0.10301481932401657, accuracy 0.1111111111111111\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19670)\u001b[0m Epoch 2: train loss 0.10235054045915604, accuracy 0.08888888888888889\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19669)\u001b[0m [Client 414, round 1] fit, config: {'server_round': 1, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19669)\u001b[0m Epoch 1: train loss 0.1028142049908638, accuracy 0.13333333333333333\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19670)\u001b[0m Epoch 3: train loss 0.1013292595744133, accuracy 0.2222222222222222\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19669)\u001b[0m Epoch 2: train loss 0.10189279168844223, accuracy 0.15555555555555556\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19669)\u001b[0m Epoch 3: train loss 0.10162805020809174, accuracy 0.2222222222222222\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m [Client 492, round 1] fit, config: {'server_round': 1, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m Epoch 1: train loss 0.1021190881729126, accuracy 0.17777777777777778\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19675)\u001b[0m [Client 351, round 1] fit, config: {'server_round': 1, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m Epoch 2: train loss 0.10105717927217484, accuracy 0.2\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m Epoch 3: train loss 0.10047420859336853, accuracy 0.28888888888888886\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19671)\u001b[0m [Client 804, round 1] fit, config: {'server_round': 1, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19675)\u001b[0m Epoch 1: train loss 0.10257131606340408, accuracy 0.08888888888888889\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19675)\u001b[0m Epoch 2: train loss 0.10173418372869492, accuracy 0.06666666666666667\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19671)\u001b[0m Epoch 1: train loss 0.10264246165752411, accuracy 0.08888888888888889\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19671)\u001b[0m Epoch 2: train loss 0.10148841887712479, accuracy 0.13333333333333333\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19675)\u001b[0m Epoch 3: train loss 0.10143472999334335, accuracy 0.2\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19671)\u001b[0m Epoch 3: train loss 0.10118274390697479, accuracy 0.24444444444444444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m E0227 15:56:56.812492600   20440 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {\"created\":\"@1677509816.812473200\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":202,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "DEBUG flwr 2023-02-27 15:56:57,682 | server.py:229 | fit_round 1 received 25 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=20321)\u001b[0m [Client 79, round 1] fit, config: {'server_round': 1, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20321)\u001b[0m Epoch 1: train loss 0.10251282900571823, accuracy 0.15555555555555556\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20321)\u001b[0m Epoch 2: train loss 0.10206764936447144, accuracy 0.17777777777777778\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20321)\u001b[0m Epoch 3: train loss 0.10134074091911316, accuracy 0.17777777777777778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING flwr 2023-02-27 15:56:58,057 | fedavg.py:242 | No fit_metrics_aggregation_fn provided\n",
      "DEBUG flwr 2023-02-27 15:56:58,058 | server.py:165 | evaluate_round 1: strategy sampled 50 clients (out of 1000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20321)\u001b[0m [Client 363] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20321)\u001b[0m [Client 274] evaluate, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(raylet)\u001b[0m Spilled 4426 MiB, 64 objects, write throughput 133 MiB/s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=19676)\u001b[0m [Client 94] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=19669)\u001b[0m [Client 482] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20327)\u001b[0m [Client 90] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=19670)\u001b[0m [Client 129] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=19676)\u001b[0m [Client 468] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=19669)\u001b[0m [Client 734] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20327)\u001b[0m [Client 503] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=19670)\u001b[0m [Client 907] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20440)\u001b[0m [Client 408] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20526)\u001b[0m [Client 7] evaluate, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m E0227 15:57:32.203798400   20521 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {\"created\":\"@1677509852.203768500\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":202,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m E0227 15:57:32.232329600   20526 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {\"created\":\"@1677509852.232226600\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":202,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m E0227 15:57:32.337754900   20527 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {\"created\":\"@1677509852.337718700\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":202,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-27 15:57:37,552 E 19577 19577] (raylet) node_manager.cc:3097: 5 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 23822b6a08ac13c06b82c01db60956472938564608a9004253685752, IP: 10.246.68.42) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.246.68.42`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=19670)\u001b[0m [Client 997] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=19670)\u001b[0m [Client 272] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=19676)\u001b[0m [Client 582] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20327)\u001b[0m [Client 112] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=19670)\u001b[0m [Client 357] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=19669)\u001b[0m [Client 823] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=19669)\u001b[0m [Client 493] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=19669)\u001b[0m [Client 875] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=19669)\u001b[0m [Client 397] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20638)\u001b[0m [Client 948] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=19669)\u001b[0m [Client 485] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=19669)\u001b[0m [Client 285] evaluate, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m E0227 15:58:02.518128500   20638 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {\"created\":\"@1677509882.518096200\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":202,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=19669)\u001b[0m [Client 407] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20638)\u001b[0m [Client 120] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=19670)\u001b[0m [Client 150] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20638)\u001b[0m [Client 511] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20638)\u001b[0m [Client 950] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=19676)\u001b[0m [Client 343] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20638)\u001b[0m [Client 44] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=19670)\u001b[0m [Client 22] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20327)\u001b[0m [Client 292] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=19670)\u001b[0m [Client 566] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=19670)\u001b[0m [Client 901] evaluate, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(raylet)\u001b[0m Spilled 8848 MiB, 108 objects, write throughput 163 MiB/s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=19670)\u001b[0m [Client 463] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=19670)\u001b[0m [Client 642] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=19670)\u001b[0m [Client 774] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=19670)\u001b[0m [Client 637] evaluate, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-27 15:58:37,554 E 19577 19577] (raylet) node_manager.cc:3097: 4 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 23822b6a08ac13c06b82c01db60956472938564608a9004253685752, IP: 10.246.68.42) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.246.68.42`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20327)\u001b[0m [Client 101] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20638)\u001b[0m [Client 495] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=19670)\u001b[0m [Client 131] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=19676)\u001b[0m [Client 605] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20327)\u001b[0m [Client 588] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=19670)\u001b[0m [Client 456] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20638)\u001b[0m [Client 675] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=19676)\u001b[0m [Client 756] evaluate, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m E0227 15:58:52.112224100   20698 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {\"created\":\"@1677509932.112200200\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":202,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m E0227 15:58:52.162060000   20703 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {\"created\":\"@1677509932.162033000\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":202,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m E0227 15:58:52.188031200   20699 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {\"created\":\"@1677509932.187987500\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":202,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m E0227 15:58:52.215237700   20694 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {\"created\":\"@1677509932.215198200\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":202,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "DEBUG flwr 2023-02-27 15:58:57,443 | server.py:179 | evaluate_round 1 received 50 results and 0 failures\n",
      "WARNING flwr 2023-02-27 15:58:57,445 | fedavg.py:273 | No evaluate_metrics_aggregation_fn provided\n",
      "DEBUG flwr 2023-02-27 15:58:57,446 | server.py:215 | fit_round 2: strategy sampled 25 clients (out of 1000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20698)\u001b[0m [Client 882] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20703)\u001b[0m [Client 293] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20699)\u001b[0m [Client 921] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20703)\u001b[0m [Client 599, round 2] fit, config: {'server_round': 2, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20703)\u001b[0m Epoch 1: train loss 0.10148078203201294, accuracy 0.13333333333333333\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20703)\u001b[0m Epoch 2: train loss 0.10104537755250931, accuracy 0.24444444444444444\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20703)\u001b[0m Epoch 3: train loss 0.09978871047496796, accuracy 0.2222222222222222\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20699)\u001b[0m [Client 3, round 2] fit, config: {'server_round': 2, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20699)\u001b[0m Epoch 1: train loss 0.10245810449123383, accuracy 0.06666666666666667\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20699)\u001b[0m Epoch 2: train loss 0.10116440802812576, accuracy 0.17777777777777778\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20699)\u001b[0m Epoch 3: train loss 0.10026058554649353, accuracy 0.2222222222222222\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20699)\u001b[0m [Client 621, round 2] fit, config: {'server_round': 2, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20699)\u001b[0m Epoch 1: train loss 0.10195997357368469, accuracy 0.17777777777777778\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20699)\u001b[0m Epoch 2: train loss 0.10078136622905731, accuracy 0.26666666666666666\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20699)\u001b[0m Epoch 3: train loss 0.10064190626144409, accuracy 0.24444444444444444\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20698)\u001b[0m [Client 902, round 2] fit, config: {'server_round': 2, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20698)\u001b[0m Epoch 1: train loss 0.10251910239458084, accuracy 0.13333333333333333\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20698)\u001b[0m Epoch 2: train loss 0.10136174410581589, accuracy 0.28888888888888886\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20698)\u001b[0m Epoch 3: train loss 0.09951900690793991, accuracy 0.26666666666666666\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m [Client 249, round 2] fit, config: {'server_round': 2, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m Epoch 1: train loss 0.1020912453532219, accuracy 0.08888888888888889\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m Epoch 2: train loss 0.10135748982429504, accuracy 0.08888888888888889\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m Epoch 3: train loss 0.09928614646196365, accuracy 0.17777777777777778\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19670)\u001b[0m [Client 813, round 2] fit, config: {'server_round': 2, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19670)\u001b[0m Epoch 1: train loss 0.1019362136721611, accuracy 0.1111111111111111\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20638)\u001b[0m [Client 991, round 2] fit, config: {'server_round': 2, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20698)\u001b[0m [Client 706, round 2] fit, config: {'server_round': 2, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19670)\u001b[0m Epoch 2: train loss 0.10076672583818436, accuracy 0.2222222222222222\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20638)\u001b[0m Epoch 1: train loss 0.10264003276824951, accuracy 0.13333333333333333\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19670)\u001b[0m Epoch 3: train loss 0.09995780885219574, accuracy 0.2222222222222222\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20638)\u001b[0m Epoch 2: train loss 0.10164476931095123, accuracy 0.15555555555555556\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20638)\u001b[0m Epoch 3: train loss 0.10086958855390549, accuracy 0.15555555555555556\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20698)\u001b[0m Epoch 1: train loss 0.102281853556633, accuracy 0.06666666666666667\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20698)\u001b[0m Epoch 2: train loss 0.10101085156202316, accuracy 0.37777777777777777\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20698)\u001b[0m Epoch 3: train loss 0.09910470992326736, accuracy 0.24444444444444444\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20327)\u001b[0m [Client 815, round 2] fit, config: {'server_round': 2, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20327)\u001b[0m Epoch 1: train loss 0.10242163389921188, accuracy 0.15555555555555556\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20327)\u001b[0m Epoch 2: train loss 0.10148762911558151, accuracy 0.15555555555555556\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20327)\u001b[0m Epoch 3: train loss 0.10054107010364532, accuracy 0.2222222222222222\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20638)\u001b[0m [Client 611, round 2] fit, config: {'server_round': 2, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20638)\u001b[0m Epoch 1: train loss 0.10193996131420135, accuracy 0.13333333333333333\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20698)\u001b[0m [Client 802, round 2] fit, config: {'server_round': 2, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20694)\u001b[0m [Client 573, round 2] fit, config: {'server_round': 2, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20638)\u001b[0m Epoch 2: train loss 0.10101745277643204, accuracy 0.26666666666666666\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20638)\u001b[0m Epoch 3: train loss 0.09971053153276443, accuracy 0.26666666666666666\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20698)\u001b[0m Epoch 1: train loss 0.102469801902771, accuracy 0.2\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20698)\u001b[0m Epoch 2: train loss 0.10174191743135452, accuracy 0.15555555555555556\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20694)\u001b[0m Epoch 1: train loss 0.10258795320987701, accuracy 0.08888888888888889\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19670)\u001b[0m [Client 422, round 2] fit, config: {'server_round': 2, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19670)\u001b[0m Epoch 1: train loss 0.10188812762498856, accuracy 0.1111111111111111\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20698)\u001b[0m Epoch 3: train loss 0.1001630648970604, accuracy 0.15555555555555556\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20694)\u001b[0m Epoch 2: train loss 0.10160765051841736, accuracy 0.24444444444444444\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20694)\u001b[0m Epoch 3: train loss 0.10032382607460022, accuracy 0.17777777777777778\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19670)\u001b[0m Epoch 2: train loss 0.10168077796697617, accuracy 0.2\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19670)\u001b[0m Epoch 3: train loss 0.1001863181591034, accuracy 0.24444444444444444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-27 15:59:37,555 E 19577 19577] (raylet) node_manager.cc:3097: 2 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 23822b6a08ac13c06b82c01db60956472938564608a9004253685752, IP: 10.246.68.42) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.246.68.42`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=19670)\u001b[0m [Client 480, round 2] fit, config: {'server_round': 2, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19670)\u001b[0m Epoch 1: train loss 0.10202839225530624, accuracy 0.1111111111111111\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19670)\u001b[0m Epoch 2: train loss 0.10137329250574112, accuracy 0.2\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19670)\u001b[0m Epoch 3: train loss 0.10073395073413849, accuracy 0.35555555555555557\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20694)\u001b[0m [Client 212, round 2] fit, config: {'server_round': 2, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20698)\u001b[0m [Client 262, round 2] fit, config: {'server_round': 2, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20694)\u001b[0m Epoch 1: train loss 0.10224051028490067, accuracy 0.13333333333333333\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20698)\u001b[0m Epoch 1: train loss 0.10184305161237717, accuracy 0.17777777777777778\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20694)\u001b[0m Epoch 2: train loss 0.10190055519342422, accuracy 0.15555555555555556\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20327)\u001b[0m [Client 374, round 2] fit, config: {'server_round': 2, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20698)\u001b[0m Epoch 2: train loss 0.10118431597948074, accuracy 0.17777777777777778\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20698)\u001b[0m Epoch 3: train loss 0.1004410833120346, accuracy 0.2\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20694)\u001b[0m Epoch 3: train loss 0.10098665952682495, accuracy 0.17777777777777778\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20327)\u001b[0m Epoch 1: train loss 0.1016024723649025, accuracy 0.2\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20327)\u001b[0m Epoch 2: train loss 0.10053268820047379, accuracy 0.15555555555555556\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20638)\u001b[0m [Client 392, round 2] fit, config: {'server_round': 2, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m [Client 131, round 2] fit, config: {'server_round': 2, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20327)\u001b[0m Epoch 3: train loss 0.09972411394119263, accuracy 0.2\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20638)\u001b[0m Epoch 1: train loss 0.10316909849643707, accuracy 0.044444444444444446\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20638)\u001b[0m Epoch 2: train loss 0.10192646086215973, accuracy 0.06666666666666667\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m Epoch 1: train loss 0.10182912647724152, accuracy 0.1111111111111111\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20638)\u001b[0m Epoch 3: train loss 0.1005178838968277, accuracy 0.24444444444444444\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m Epoch 2: train loss 0.10120753943920135, accuracy 0.17777777777777778\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m Epoch 3: train loss 0.10083857923746109, accuracy 0.17777777777777778\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20698)\u001b[0m [Client 275, round 2] fit, config: {'server_round': 2, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20698)\u001b[0m Epoch 1: train loss 0.10204529017210007, accuracy 0.13333333333333333\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20698)\u001b[0m Epoch 2: train loss 0.10032249987125397, accuracy 0.15555555555555556\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20698)\u001b[0m Epoch 3: train loss 0.09994860738515854, accuracy 0.28888888888888886\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20694)\u001b[0m [Client 999, round 2] fit, config: {'server_round': 2, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m [Client 671, round 2] fit, config: {'server_round': 2, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20694)\u001b[0m Epoch 1: train loss 0.10264212638139725, accuracy 0.1111111111111111\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m Epoch 1: train loss 0.10267346352338791, accuracy 0.08888888888888889\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20327)\u001b[0m [Client 874, round 2] fit, config: {'server_round': 2, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m Epoch 2: train loss 0.10181106626987457, accuracy 0.1111111111111111\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m Epoch 3: train loss 0.10131695866584778, accuracy 0.13333333333333333\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20327)\u001b[0m Epoch 1: train loss 0.10250811278820038, accuracy 0.06666666666666667\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20327)\u001b[0m Epoch 2: train loss 0.10131850838661194, accuracy 0.17777777777777778\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20327)\u001b[0m Epoch 3: train loss 0.09932588040828705, accuracy 0.15555555555555556\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20638)\u001b[0m [Client 430, round 2] fit, config: {'server_round': 2, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20638)\u001b[0m Epoch 1: train loss 0.10232114046812057, accuracy 0.1111111111111111\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20694)\u001b[0m Epoch 2: train loss 0.10154753923416138, accuracy 0.17777777777777778\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20638)\u001b[0m Epoch 2: train loss 0.1015099510550499, accuracy 0.2222222222222222\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20694)\u001b[0m Epoch 3: train loss 0.10055922716856003, accuracy 0.15555555555555556\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20638)\u001b[0m Epoch 3: train loss 0.09974530339241028, accuracy 0.17777777777777778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m E0227 15:59:56.129624400   20886 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {\"created\":\"@1677509996.129589000\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":202,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m E0227 15:59:56.147463800   20906 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {\"created\":\"@1677509996.147420200\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":202,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "DEBUG flwr 2023-02-27 16:00:00,553 | server.py:229 | fit_round 2 received 25 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=20906)\u001b[0m [Client 978, round 2] fit, config: {'server_round': 2, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20906)\u001b[0m Epoch 1: train loss 0.10216633230447769, accuracy 0.13333333333333333\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20906)\u001b[0m Epoch 2: train loss 0.10124514997005463, accuracy 0.1111111111111111\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20906)\u001b[0m Epoch 3: train loss 0.10028143227100372, accuracy 0.3333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-02-27 16:00:00,684 | server.py:165 | evaluate_round 2: strategy sampled 50 clients (out of 1000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20638)\u001b[0m [Client 235] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20638)\u001b[0m [Client 698] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20886)\u001b[0m [Client 399] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20886)\u001b[0m [Client 930] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20694)\u001b[0m [Client 742] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20327)\u001b[0m [Client 45] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20327)\u001b[0m [Client 372] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20327)\u001b[0m [Client 476] evaluate, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-27 16:00:37,558 E 19577 19577] (raylet) node_manager.cc:3097: 4 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 23822b6a08ac13c06b82c01db60956472938564608a9004253685752, IP: 10.246.68.42) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.246.68.42`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20327)\u001b[0m [Client 795] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20694)\u001b[0m [Client 210] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=19676)\u001b[0m [Client 552] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20698)\u001b[0m [Client 653] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=19676)\u001b[0m [Client 283] evaluate, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m E0227 16:00:52.746526300   21033 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {\"created\":\"@1677510052.746487700\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":202,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m E0227 16:00:52.756731800   21035 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {\"created\":\"@1677510052.756696600\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":202,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m E0227 16:00:52.805626000   21029 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {\"created\":\"@1677510052.805573300\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":202,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=19676)\u001b[0m [Client 486] evaluate, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(raylet)\u001b[0m Spilled 16667 MiB, 213 objects, write throughput 171 MiB/s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=19676)\u001b[0m [Client 970] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20698)\u001b[0m [Client 31] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20694)\u001b[0m [Client 620] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21033)\u001b[0m [Client 540] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21035)\u001b[0m [Client 457] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21029)\u001b[0m [Client 50] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21029)\u001b[0m [Client 186] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21029)\u001b[0m [Client 585] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21033)\u001b[0m [Client 896] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21035)\u001b[0m [Client 137] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21029)\u001b[0m [Client 760] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21033)\u001b[0m [Client 410] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21035)\u001b[0m [Client 265] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21029)\u001b[0m [Client 806] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21035)\u001b[0m [Client 536] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20698)\u001b[0m [Client 238] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20694)\u001b[0m [Client 352] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20698)\u001b[0m [Client 886] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20698)\u001b[0m [Client 971] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20698)\u001b[0m [Client 57] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20694)\u001b[0m [Client 651] evaluate, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-27 16:01:37,559 E 19577 19577] (raylet) node_manager.cc:3097: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 23822b6a08ac13c06b82c01db60956472938564608a9004253685752, IP: 10.246.68.42) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.246.68.42`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20698)\u001b[0m [Client 445] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20694)\u001b[0m [Client 982] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20698)\u001b[0m [Client 6] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20698)\u001b[0m [Client 134] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=19676)\u001b[0m [Client 654] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20694)\u001b[0m [Client 830] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21035)\u001b[0m [Client 304] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20698)\u001b[0m [Client 401] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21029)\u001b[0m [Client 136] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21029)\u001b[0m [Client 89] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21035)\u001b[0m [Client 419] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=19676)\u001b[0m [Client 764] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20698)\u001b[0m [Client 761] evaluate, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-02-27 16:01:54,424 | server.py:179 | evaluate_round 2 received 50 results and 0 failures\n",
      "DEBUG flwr 2023-02-27 16:01:54,428 | server.py:215 | fit_round 3: strategy sampled 25 clients (out of 1000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20327)\u001b[0m [Client 561] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20694)\u001b[0m [Client 861] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20694)\u001b[0m [Client 377, round 3] fit, config: {'server_round': 3, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20694)\u001b[0m Epoch 1: train loss 0.10097963362932205, accuracy 0.15555555555555556\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20694)\u001b[0m Epoch 2: train loss 0.09817608445882797, accuracy 0.26666666666666666\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20694)\u001b[0m Epoch 3: train loss 0.09615800529718399, accuracy 0.28888888888888886\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m [Client 384, round 3] fit, config: {'server_round': 3, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m Epoch 1: train loss 0.10175963491201401, accuracy 0.08888888888888889\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m Epoch 2: train loss 0.10012194514274597, accuracy 0.15555555555555556\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m Epoch 3: train loss 0.09829230606555939, accuracy 0.24444444444444444\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m [Client 301, round 3] fit, config: {'server_round': 3, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m Epoch 1: train loss 0.10155805200338364, accuracy 0.15555555555555556\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m Epoch 2: train loss 0.10044339299201965, accuracy 0.28888888888888886\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m Epoch 3: train loss 0.09796519577503204, accuracy 0.3111111111111111\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m [Client 508, round 3] fit, config: {'server_round': 3, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m Epoch 1: train loss 0.10161853581666946, accuracy 0.13333333333333333\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m Epoch 2: train loss 0.09956257790327072, accuracy 0.17777777777777778\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m Epoch 3: train loss 0.09747350215911865, accuracy 0.2222222222222222\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m [Client 311, round 3] fit, config: {'server_round': 3, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m Epoch 1: train loss 0.10220519453287125, accuracy 0.08888888888888889\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m Epoch 2: train loss 0.10117922723293304, accuracy 0.13333333333333333\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m Epoch 3: train loss 0.10019025951623917, accuracy 0.26666666666666666\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21029)\u001b[0m [Client 777, round 3] fit, config: {'server_round': 3, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20698)\u001b[0m [Client 851, round 3] fit, config: {'server_round': 3, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20698)\u001b[0m Epoch 1: train loss 0.1016610860824585, accuracy 0.13333333333333333\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21035)\u001b[0m [Client 723, round 3] fit, config: {'server_round': 3, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21035)\u001b[0m Epoch 1: train loss 0.10155846178531647, accuracy 0.17777777777777778\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21029)\u001b[0m Epoch 1: train loss 0.1018138900399208, accuracy 0.1111111111111111\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21029)\u001b[0m Epoch 2: train loss 0.1001715138554573, accuracy 0.28888888888888886\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20698)\u001b[0m Epoch 2: train loss 0.09987537562847137, accuracy 0.2222222222222222\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20698)\u001b[0m Epoch 3: train loss 0.0989881232380867, accuracy 0.24444444444444444\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21035)\u001b[0m Epoch 2: train loss 0.10002495348453522, accuracy 0.37777777777777777\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21035)\u001b[0m Epoch 3: train loss 0.09808807075023651, accuracy 0.37777777777777777\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21029)\u001b[0m Epoch 3: train loss 0.09962834417819977, accuracy 0.26666666666666666\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20698)\u001b[0m [Client 78, round 3] fit, config: {'server_round': 3, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m [Client 337, round 3] fit, config: {'server_round': 3, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m Epoch 1: train loss 0.10174579918384552, accuracy 0.1111111111111111\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m Epoch 2: train loss 0.09976339340209961, accuracy 0.28888888888888886\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21035)\u001b[0m [Client 327, round 3] fit, config: {'server_round': 3, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m Epoch 3: train loss 0.09664256125688553, accuracy 0.26666666666666666\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21035)\u001b[0m Epoch 1: train loss 0.1019749641418457, accuracy 0.17777777777777778\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20698)\u001b[0m Epoch 1: train loss 0.1019023060798645, accuracy 0.1111111111111111\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21035)\u001b[0m Epoch 2: train loss 0.10102064162492752, accuracy 0.2222222222222222\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20698)\u001b[0m Epoch 2: train loss 0.10017931461334229, accuracy 0.28888888888888886\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21035)\u001b[0m Epoch 3: train loss 0.09981848299503326, accuracy 0.26666666666666666\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20698)\u001b[0m Epoch 3: train loss 0.09965614974498749, accuracy 0.26666666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m E0227 16:02:32.937896800   21188 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {\"created\":\"@1677510152.937863300\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":202,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m E0227 16:02:32.938174000   21199 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {\"created\":\"@1677510152.938141300\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":202,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m E0227 16:02:32.949628100   21198 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {\"created\":\"@1677510152.949593400\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":202,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-27 16:02:37,561 E 19577 19577] (raylet) node_manager.cc:3097: 2 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 23822b6a08ac13c06b82c01db60956472938564608a9004253685752, IP: 10.246.68.42) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.246.68.42`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=21188)\u001b[0m [Client 909, round 3] fit, config: {'server_round': 3, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21188)\u001b[0m Epoch 1: train loss 0.1013437882065773, accuracy 0.13333333333333333\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21199)\u001b[0m [Client 721, round 3] fit, config: {'server_round': 3, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21199)\u001b[0m Epoch 1: train loss 0.1020599752664566, accuracy 0.06666666666666667\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21198)\u001b[0m [Client 401, round 3] fit, config: {'server_round': 3, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21198)\u001b[0m Epoch 1: train loss 0.10167784243822098, accuracy 0.13333333333333333\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21188)\u001b[0m Epoch 2: train loss 0.09942928701639175, accuracy 0.24444444444444444\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21188)\u001b[0m Epoch 3: train loss 0.0985245630145073, accuracy 0.2222222222222222\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21199)\u001b[0m Epoch 2: train loss 0.09925474226474762, accuracy 0.24444444444444444\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21199)\u001b[0m Epoch 3: train loss 0.09640500694513321, accuracy 0.3111111111111111\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21198)\u001b[0m Epoch 2: train loss 0.10009617358446121, accuracy 0.2\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21198)\u001b[0m Epoch 3: train loss 0.09680211544036865, accuracy 0.2\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20698)\u001b[0m [Client 110, round 3] fit, config: {'server_round': 3, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m [Client 181, round 3] fit, config: {'server_round': 3, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20698)\u001b[0m Epoch 1: train loss 0.10092444717884064, accuracy 0.17777777777777778\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m Epoch 1: train loss 0.10131971538066864, accuracy 0.1111111111111111\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20698)\u001b[0m Epoch 2: train loss 0.09738123416900635, accuracy 0.3111111111111111\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21188)\u001b[0m [Client 207, round 3] fit, config: {'server_round': 3, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21199)\u001b[0m [Client 836, round 3] fit, config: {'server_round': 3, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21198)\u001b[0m [Client 919, round 3] fit, config: {'server_round': 3, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m Epoch 2: train loss 0.098678357899189, accuracy 0.26666666666666666\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=19676)\u001b[0m Epoch 3: train loss 0.0963059514760971, accuracy 0.35555555555555557\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20698)\u001b[0m Epoch 3: train loss 0.09624267369508743, accuracy 0.3111111111111111\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21035)\u001b[0m [Client 148, round 3] fit, config: {'server_round': 3, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21035)\u001b[0m Epoch 1: train loss 0.10142452269792557, accuracy 0.15555555555555556\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21029)\u001b[0m [Client 419, round 3] fit, config: {'server_round': 3, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21029)\u001b[0m Epoch 1: train loss 0.10216348618268967, accuracy 0.1111111111111111\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21188)\u001b[0m Epoch 1: train loss 0.10259540379047394, accuracy 0.06666666666666667\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21188)\u001b[0m Epoch 2: train loss 0.10126809030771255, accuracy 0.17777777777777778\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21199)\u001b[0m \n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21199)\u001b[0m Epoch 1: train loss 0.10192228108644485, accuracy 0.15555555555555556\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21198)\u001b[0m Epoch 1: train loss 0.10214316099882126, accuracy 0.13333333333333333\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21198)\u001b[0m Epoch 2: train loss 0.10041635483503342, accuracy 0.2\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21035)\u001b[0m Epoch 2: train loss 0.099330373108387, accuracy 0.13333333333333333\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21035)\u001b[0m Epoch 3: train loss 0.09692465513944626, accuracy 0.28888888888888886\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21029)\u001b[0m Epoch 2: train loss 0.1009569838643074, accuracy 0.2222222222222222\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21029)\u001b[0m Epoch 3: train loss 0.09972057491540909, accuracy 0.24444444444444444\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21188)\u001b[0m Epoch 3: train loss 0.10022231936454773, accuracy 0.2222222222222222\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21199)\u001b[0m Epoch 2: train loss 0.10102538019418716, accuracy 0.15555555555555556\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21199)\u001b[0m Epoch 3: train loss 0.09945445507764816, accuracy 0.26666666666666666\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21198)\u001b[0m Epoch 3: train loss 0.09893321990966797, accuracy 0.24444444444444444\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21035)\u001b[0m [Client 126, round 3] fit, config: {'server_round': 3, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21035)\u001b[0m Epoch 1: train loss 0.10210797935724258, accuracy 0.06666666666666667\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21199)\u001b[0m [Client 64, round 3] fit, config: {'server_round': 3, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20698)\u001b[0m [Client 955, round 3] fit, config: {'server_round': 3, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20698)\u001b[0m Epoch 1: train loss 0.10176414996385574, accuracy 0.13333333333333333\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21035)\u001b[0m Epoch 2: train loss 0.10027830302715302, accuracy 0.17777777777777778\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21035)\u001b[0m Epoch 3: train loss 0.09938269853591919, accuracy 0.2222222222222222\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21199)\u001b[0m Epoch 1: train loss 0.10081424564123154, accuracy 0.15555555555555556\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21199)\u001b[0m Epoch 2: train loss 0.09940770268440247, accuracy 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-02-27 16:02:52,322 | server.py:229 | fit_round 3 received 25 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_fit pid=20698)\u001b[0m Epoch 2: train loss 0.10044965893030167, accuracy 0.2\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=20698)\u001b[0m Epoch 3: train loss 0.09850294142961502, accuracy 0.24444444444444444\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21199)\u001b[0m Epoch 3: train loss 0.09857697039842606, accuracy 0.2222222222222222\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21198)\u001b[0m [Client 946, round 3] fit, config: {'server_round': 3, 'local_epochs': 3}\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21198)\u001b[0m Epoch 1: train loss 0.10195352882146835, accuracy 0.13333333333333333\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21198)\u001b[0m Epoch 2: train loss 0.10066024214029312, accuracy 0.24444444444444444\n",
      "\u001b[2m\u001b[36m(launch_and_fit pid=21198)\u001b[0m Epoch 3: train loss 0.09983587265014648, accuracy 0.28888888888888886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-02-27 16:02:52,555 | server.py:165 | evaluate_round 3: strategy sampled 50 clients (out of 1000)\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m E0227 16:02:55.997637600   21331 socket_utils_common_posix.cc:223] check for SO_REUSEPORT: {\"created\":\"@1677510175.997601500\",\"description\":\"Protocol not available\",\"errno\":92,\"file\":\"external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc\",\"file_line\":202,\"os_error\":\"Protocol not available\",\"syscall\":\"getsockopt(SO_REUSEPORT)\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21198)\u001b[0m [Client 398] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21198)\u001b[0m [Client 850] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20698)\u001b[0m [Client 200] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21198)\u001b[0m [Client 652] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21331)\u001b[0m [Client 123] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21331)\u001b[0m [Client 797] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21198)\u001b[0m [Client 154] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21198)\u001b[0m [Client 851] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21198)\u001b[0m [Client 803] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21331)\u001b[0m [Client 12] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21331)\u001b[0m [Client 299] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20698)\u001b[0m [Client 497] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21199)\u001b[0m [Client 304] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21198)\u001b[0m [Client 831] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21198)\u001b[0m [Client 685] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20698)\u001b[0m [Client 506] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21198)\u001b[0m [Client 918] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21331)\u001b[0m [Client 828] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21035)\u001b[0m [Client 10] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21199)\u001b[0m [Client 676] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21199)\u001b[0m [Client 434] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21199)\u001b[0m [Client 560] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21035)\u001b[0m [Client 204] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21199)\u001b[0m [Client 268] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21035)\u001b[0m [Client 411] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21199)\u001b[0m [Client 985] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21198)\u001b[0m [Client 366] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21331)\u001b[0m [Client 37] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20698)\u001b[0m [Client 334] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20698)\u001b[0m [Client 925] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20698)\u001b[0m [Client 591] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20698)\u001b[0m [Client 905] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21199)\u001b[0m [Client 673] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21198)\u001b[0m [Client 305] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21035)\u001b[0m [Client 463] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21331)\u001b[0m [Client 438] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20698)\u001b[0m [Client 294] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=19676)\u001b[0m [Client 527] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20698)\u001b[0m [Client 475] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=19676)\u001b[0m [Client 26] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21035)\u001b[0m [Client 278] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21198)\u001b[0m [Client 394] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21199)\u001b[0m [Client 548] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21331)\u001b[0m [Client 763] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=19676)\u001b[0m [Client 323] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21035)\u001b[0m [Client 684] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=20698)\u001b[0m [Client 576] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21188)\u001b[0m [Client 952] evaluate, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2023-02-27 16:04:34,750 | server.py:179 | evaluate_round 3 received 50 results and 0 failures\n",
      "INFO flwr 2023-02-27 16:04:34,770 | server.py:144 | FL finished in 533.6758999000003\n",
      "INFO flwr 2023-02-27 16:04:34,772 | app.py:202 | app_fit: losses_distributed [(1, 0.4597410078048705), (2, 0.45852434730529773), (3, 0.4515949525833129)]\n",
      "INFO flwr 2023-02-27 16:04:34,774 | app.py:203 | app_fit: metrics_distributed {}\n",
      "INFO flwr 2023-02-27 16:04:34,776 | app.py:204 | app_fit: losses_centralized []\n",
      "INFO flwr 2023-02-27 16:04:34,777 | app.py:205 | app_fit: metrics_centralized {}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21199)\u001b[0m [Client 265] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(launch_and_evaluate pid=21198)\u001b[0m [Client 138] evaluate, config: {}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "History (loss, distributed):\n",
       "\tround 1: 0.4597410078048705\n",
       "\tround 2: 0.45852434730529773\n",
       "\tround 3: 0.4515949525833129"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-27 16:10:37,631 E 19577 19577] (raylet) node_manager.cc:3097: 3 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 23822b6a08ac13c06b82c01db60956472938564608a9004253685752, IP: 10.246.68.42) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.246.68.42`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-27 16:11:37,638 E 19577 19577] (raylet) node_manager.cc:3097: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 23822b6a08ac13c06b82c01db60956472938564608a9004253685752, IP: 10.246.68.42) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.246.68.42`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-27 16:12:37,639 E 19577 19577] (raylet) node_manager.cc:3097: 7 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 23822b6a08ac13c06b82c01db60956472938564608a9004253685752, IP: 10.246.68.42) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.246.68.42`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-27 16:13:37,645 E 19577 19577] (raylet) node_manager.cc:3097: 5 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 23822b6a08ac13c06b82c01db60956472938564608a9004253685752, IP: 10.246.68.42) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.246.68.42`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    }
   ],
   "source": [
    "def fit_config(server_round: int):\n",
    "    config = {\n",
    "        \"server_round\": server_round,\n",
    "        \"local_epochs\": 3,\n",
    "    }\n",
    "    return config\n",
    "\n",
    "\n",
    "strategy = fl.server.strategy.FedAvg(\n",
    "    fraction_fit=0.025,  # Train on 25 clients (each round)\n",
    "    fraction_evaluate=0.05,  # Evaluate on 50 clients (each round)\n",
    "    min_fit_clients=20,\n",
    "    min_evaluate_clients=40,\n",
    "    min_available_clients=NUM_CLIENTS,\n",
    "    initial_parameters=fl.common.ndarrays_to_parameters(get_parameters(Net())),\n",
    "    on_fit_config_fn=fit_config,\n",
    ")\n",
    "\n",
    "fl.simulation.start_simulation(\n",
    "    client_fn=client_fn,\n",
    "    num_clients=NUM_CLIENTS,\n",
    "    config=fl.server.ServerConfig(num_rounds=3),  # Just three rounds\n",
    "    strategy=strategy,\n",
    "    client_resources=client_resources,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "In this notebook, we've seen how we can gradually enhance our system by customizing the strategy, initializing parameters on the server side, choosing a different strategy, and evaluating models on the server-side. That's quite a bit of flexibility with so little code, right?\n",
    "\n",
    "In the later sections, we've seen how we can communicate arbitrary values between server and clients to fully customize client-side execution. With that capability, we built a large-scale Federated Learning simulation using the Flower Virtual Client Engine and ran an experiment involving 1000 clients in the same workload - all in a Jupyter Notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "Before you continue, make sure to join the Flower community on Slack: [Join Slack](https://flower.dev/join-slack/)\n",
    "\n",
    "There's a dedicated `#questions` channel if you need help, but we'd also love to hear who you are in `#introductions`!\n",
    "\n",
    "The [Flower Federated Learning Tutorial - Part 3 [WIP]](https://flower.dev/docs/tutorial/Flower-3-Building-a-Strategy-PyTorch.html) shows how to build a fully custom `Strategy` from scratch."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Flower-2-Strategies-in-FL-PyTorch.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "flower1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "612ab0d2b96a1aba50cafbd617a760c726240d95c61e1992fc57227b8c37e978"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
